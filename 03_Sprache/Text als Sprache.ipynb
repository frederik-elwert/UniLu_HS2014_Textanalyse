{
 "metadata": {
  "name": "",
  "signature": "sha256:0a82adf4bb9c88ee6fb0350c9c7823a4eba1d65f51480ae96840db9aa65ed741"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Text als Sprache"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Als *Natural Language Processing* bezeichnet man die computergest\u00fctzte Verarbeitung von nat\u00fcrlichen Sprachen, also von von Menschen gesprochenen Sprachen im Gegensatz zu Programmiersprachen. Computer sind gut darin, Programmiersprachen mit ihren starren Regeln zu verstehen, aber menschliche Sprachen mit ihren Unregelm\u00e4\u00dfigkeiten stellen immer noch eine Herausforderung dar. Die Wissenschaft, die sich mit diesen Problemen besch\u00e4ftigt, ist die Computerlinguistik.\n",
      "\n",
      "Methoden der Computerlinguistik sind mittlerweile f\u00fcr eine Vielzahl von Anwendungsbereichen zentral. So sind etwa Suchmaschinen darauf trainiert, nicht nur den exakten Text einer Suchanfrage zu finden, sondern auch andere Flexionsformen. Eine Google-Suche nach \u201eParties in Hamburg\u201c findet so z.B. auch die Seite [Nachtleben & Party \u2013 hamburg.de](http://www.hamburg.de/nachtleben-party/). Dazu muss die Suchmaschine wissen, dass \u201eParty\u201c der Singular von \u201eParties\u201c ist, und dass \u201ein\u201c als Pr\u00e4position f\u00fcr den gesuchten Inhalt nicht zwingend relevant ist.\n",
      "\n",
      "Der Ausgangspunkt f\u00fcr NLP ist dabei immer der reine Text, ohne Formatierung oder \u00e4hnliches. Solche Dateien k\u00f6nnen in Python problemlos eingelesen werden. Dabei muss allerdings die [Zeichencodierung](http://de.wikipedia.org/wiki/Zeichenkodierung) des Textes bekannt sein. In der Regel arbeitet man heutzutage mit [UTF-8](http://de.wikipedia.org/wiki/UTF-8)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Als Beispiel soll der Anfang einer Rede der deutschen Bundeskanzlerin Angela Merkel dienen. (Der Anschaulichkeit halber ist die Anrede entfernt.) Um die Datei `Rede_Rundfunk.txt` mit der Kodierung `UTF-8` zum Lesen (`r`) zu \u00f6ffnen, dient dieser Befehl:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "with codecs.open('Rede_Rundfunk.txt', 'r', 'utf-8') as infile:\n",
      "    rede = infile.read()\n",
      "print rede[0:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vor 30 Jahren waren Sie einer der Geburtshelfer des privaten Rundfunks. Ihre Worte von damals sind l\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wenn wir mit Texten arbeiten, sind wir meist aber nicht am Text als Ganzem interessiert, sondern an den W\u00f6rtern, aus denen er besteht. W\u00f6rter sind in den europ\u00e4ischen Sprachen meist durch Leerzeichen getrennt. So sehen die ersten zwanzig W\u00f6rter aus, wenn man den Text entsprechend aufspaltet:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ' -- '.join(rede.split()[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vor -- 30 -- Jahren -- waren -- Sie -- einer -- der -- Geburtshelfer -- des -- privaten -- Rundfunks. -- Ihre -- Worte -- von -- damals -- sind -- legend\u00e4r. -- Deshalb -- m\u00f6chte -- ich\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hier sieht man schon einige Probleme. Insbesondere werden hier die Satzzeichen noch beibehalten. Die verf\u00e4lschen aber das Ergebnis, denn das Wort hei\u00dft ja nicht \u201elegend\u00e4r**.**\u201c, sondern einfach \u201elegend\u00e4r\u201c. Schon bei so einfachen Aufgaben wie dem Aufspalten kann es also n\u00fctzlich sein, etwas ausgefeiltere Werkzeuge heranzuziehen. Einen einfachen Einstieg bietet dabei das Modul `TextBlob`, f\u00fcr das es auch eine auf die deutsche Sprache ausgelegte Version gibt:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob_de import TextBlobDE as TextBlob\n",
      "blob = TextBlob(rede)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print u' -- '.join(blob.words[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vor -- 30 -- Jahren -- waren -- Sie -- einer -- der -- Geburtshelfer -- des -- privaten -- Rundfunks -- Ihre -- Worte -- von -- damals -- sind -- legend\u00e4r -- Deshalb -- m\u00f6chte -- ich\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`TextBlob` stellt noch eine Reihe weiterer Methoden aus der Sprachverarbeitung bereit. So kann man etwa Worte auf ihre Grundformen zur\u00fcckf\u00fchren. Gerade bei stark flektierenden Sprachen wie dem Deutschen ist das oft n\u00fctzlich. Diesen Schritt nennt man in der Computerlinguistik Lemmatisierung."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print u' -- '.join(blob.words[0:20].lemmatize())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vor -- 30 -- Jahren -- sein -- Sie -- ein -- der -- Geburtshelfer -- des -- privat -- Rundfunks -- Ihre -- Wort -- von -- damals -- sein -- legend\u00e4r -- Deshalb -- m\u00f6gen -- ich\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Man kann sehen, dass der Schritt f\u00fcr Verben gut funktioniert. So ist aus \u201ewaren\u201c \u201esein\u201c geworden und aus \u201em\u00f6chte\u201c \u201em\u00f6gen\u201c. Bei Substantiven scheint dieser Algorithmus jedoch an seine Grenzen zu kommen: \u201eJahren\u201c ist nicht zu \u201eJahr\u201c geworden und \u201eRundfunks\u201c nicht zu \u201eRundfunk\u201c. In Anwendungsf\u00e4llen, in denen eine hohe Qualit\u00e4t der Verarbeitung wichtig ist, muss man also ggf. nach besseren Werkzeugen suchen oder die vorhandenen verbessern.\n",
      "\n",
      "F\u00fcr speziellere Anwendungen kann man auch die Grammatik eines Texts analysieren. Ein grundlegender Schritt ist dabei die Bestimming der Wortart (Substantiv, Verb, etc.), englisch \u201ePart of Speech\u201c. Hierf\u00fcr werden in der Linguistik meist bestimmte K\u00fcrzel verwendet, die von `TextBlob` sind auf [dieser Seite](http://www.clips.ua.ac.be/pages/mbsp-tags) aufgelistet."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print u' -- '.join([u'{}/{}'.format(word, tag) for word, tag in blob.pos_tags[0:20]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vor/IN -- 30/CD -- Jahren/NN -- waren/VB -- Sie/PRP -- einer/DT -- der/DT -- Geburtshelfer/NN -- des/DT -- privaten/JJ -- Rundfunks/NN -- Ihre/PRP$ -- Worte/NNS -- von/IN -- damals/RB -- sind/VB -- legend\u00e4r/JJ -- Deshalb/RB -- m\u00f6chte/VB -- ich/PRP\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dies kann etwa dazu verwendet werden, f\u00fcr bestimmte Anwendungen nur bestimmte Wortarten zu ber\u00fccksichtigen. Falls etwa nur die Substantive interessieren, lassen sie sich aufgrund der PoS-Information herausfiltern."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lemmatize_and_filter(blob, tags):\n",
      "    lemmas = blob.words.lemmatize()\n",
      "    tagged = blob.pos_tags\n",
      "    result = []\n",
      "    for lemma, (word, tag) in zip(lemmas, tagged):\n",
      "        if tag[0:2] in tags:\n",
      "            result.append(lemma)\n",
      "    return result\n",
      "\n",
      "print u' -- '.join(lemmatize_and_filter(blob, ('NN', ))[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jahren -- Geburtshelfer -- Rundfunks -- Wort -- Kellerstudio -- Ludwigshafen -- Zuschauer -- Morgen -- Januar -- Worten -- \u201emeine -- Damen -- Herren -- Moment -- Zeug -- Starts -- Fernsehveranstalters -- Bundesrepublik -- Deutschland.\u201c -- Zeugen\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Das Verfahren funktioniert in weiten Teilen, aber alle Methoden der Computerlinguistik sind mit einer gewissen Fehlerrate behaftet. Gute Verfahren haben dabei eine Genauigkeit von \u00fcber 90% \u2013 was in der Gesamtschau immer noch eine Menge Fehler sind. In diesem Fall sieht man, dass der Wortfinde-Algorithmus nicht mit den deutschen Anf\u00fchrungszeichen umgehen kann. Hier w\u00fcrde eine entsprechende Vorbereitung des Textes helfen.\n",
      "\n",
      "Ein anderer Ansatz kann sein, nicht \u00fcber die Wortarten, sondern \u00fcber eine vorgegebene Liste an W\u00f6rtern zu filtern. Je nach Anwendungszweck kann eine solche Stoppwortliste l\u00e4nger oder k\u00fcrzer sein. Ihr Ziel ist es, nicht bedeutungstragende W\u00f6rter herauszufiltern. Dies k\u00f6nnen neben Partikeln auch Hilfsverben und unspezifische Adjektive und Adverben (z.B. \u201esehr\u201c) sein."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('stopwords.txt', 'r', 'utf-8') as stopwordfile:\n",
      "    stopwords_de = [line.strip() for line in stopwordfile.readlines()]\n",
      "\n",
      "def lemmatize_and_filter2(blob, stopwords):\n",
      "    lemmas = blob.words.lemmatize()\n",
      "    result = []\n",
      "    for lemma, word in zip(lemmas, blob.words):\n",
      "        if not word.lower() in stopwords and not word.isdigit():\n",
      "            result.append(lemma)\n",
      "    return result\n",
      "\n",
      "print u' -- '.join(lemmatize_and_filter2(blob, stopwords_de)[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jahren -- Geburtshelfer -- privat -- Rundfunks -- Wort -- legend\u00e4r -- zitieren -- Kellerstudio -- Ludwigshafen -- begr\u00fcssen -- Zuschauer -- Januar -- Worten -- \u201emeine -- verehrt -- Damen -- Herren -- Moment -- Zeug -- Starts\n"
       ]
      }
     ],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}