{
 "metadata": {
  "name": "",
  "signature": "sha256:af0163c3e59030aedc96ce63f522a9224c198806b31e823b14afa75a297b3e98"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Text als Sprache"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Als *Natural Language Processing* bezeichnet man die computergest\u00fctzte Verarbeitung von nat\u00fcrlichen Sprachen, also von von Menschen gesprochenen Sprachen im Gegensatz zu Programmiersprachen. Computer sind gut darin, Programmiersprachen mit ihren starren Regeln zu verstehen, aber menschliche Sprachen mit ihren Unregelm\u00e4\u00dfigkeiten stellen immer noch eine Herausforderung dar. Die Wissenschaft, die sich mit diesen Problemen besch\u00e4ftigt, ist die Computerlinguistik.\n",
      "\n",
      "Methoden der Computerlinguistik sind mittlerweile f\u00fcr eine Vielzahl von Anwendungsbereichen zentral. So sind etwa Suchmaschinen darauf trainiert, nicht nur den exakten Text einer Suchanfrage zu finden, sondern auch andere Flexionsformen. Eine Google-Suche nach \u201eParties in Hamburg\u201c findet so z.B. auch die Seite [Nachtleben & Party \u2013 hamburg.de](http://www.hamburg.de/nachtleben-party/). Dazu muss die Suchmaschine wissen, dass \u201eParty\u201c der Singular von \u201eParties\u201c ist, und dass \u201ein\u201c als Pr\u00e4position f\u00fcr den gesuchten Inhalt nicht zwingend relevant ist.\n",
      "\n",
      "Der Ausgangspunkt f\u00fcr NLP ist dabei immer der reine Text, ohne Formatierung oder \u00e4hnliches. Solche Dateien k\u00f6nnen in Python problemlos eingelesen werden. Dabei muss allerdings die [Zeichencodierung](http://de.wikipedia.org/wiki/Zeichenkodierung) des Textes bekannt sein. In der Regel arbeitet man heutzutage mit [UTF-8](http://de.wikipedia.org/wiki/UTF-8)."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Als Beispiel soll der Anfang einer Rede der deutschen Bundeskanzlerin Angela Merkel dienen. (Der Anschaulichkeit halber ist die Anrede entfernt.) Um die Datei `Rede_Rundfunk.txt` mit der Kodierung `UTF-8` zum Lesen (`r`) zu \u00f6ffnen, dient dieser Befehl:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import codecs\n",
      "with codecs.open('Rede_Rundfunk.txt', 'r', 'utf-8') as infile:\n",
      "    rede = infile.read()\n",
      "print rede[0:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vor 30 Jahren waren Sie einer der Geburtshelfer des privaten Rundfunks. Ihre Worte von damals sind l\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wenn wir mit Texten arbeiten, sind wir meist aber nicht am Text als Ganzem interessiert, sondern an den W\u00f6rtern, aus denen er besteht. W\u00f6rter sind in den europ\u00e4ischen Sprachen meist durch Leerzeichen getrennt. So sehen die ersten zwanzig W\u00f6rter aus, wenn man den Text entsprechend aufspaltet:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print ' -- '.join(rede.split()[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vor -- 30 -- Jahren -- waren -- Sie -- einer -- der -- Geburtshelfer -- des -- privaten -- Rundfunks. -- Ihre -- Worte -- von -- damals -- sind -- legend\u00e4r. -- Deshalb -- m\u00f6chte -- ich\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Hier sieht man schon einige Probleme. Insbesondere werden hier die Satzzeichen noch beibehalten. Die verf\u00e4lschen aber das Ergebnis, denn das Wort hei\u00dft ja nicht \u201elegend\u00e4r**.**\u201c, sondern einfach \u201elegend\u00e4r\u201c. Schon bei so einfachen Aufgaben wie dem Aufspalten kann es also n\u00fctzlich sein, etwas ausgefeiltere Werkzeuge heranzuziehen. Einen einfachen Einstieg bietet dabei das Modul `TextBlob`, f\u00fcr das es auch eine auf die deutsche Sprache ausgelegte Version gibt:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob_de import TextBlobDE as TextBlob\n",
      "blob = TextBlob(rede)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print u' -- '.join(blob.words[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vor -- 30 -- Jahren -- waren -- Sie -- einer -- der -- Geburtshelfer -- des -- privaten -- Rundfunks -- Ihre -- Worte -- von -- damals -- sind -- legend\u00e4r -- Deshalb -- m\u00f6chte -- ich\n"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "`TextBlob` stellt noch eine Reihe weiterer Methoden aus der Sprachverarbeitung bereit. So kann man etwa Worte auf ihre Grundformen zur\u00fcckf\u00fchren. Gerade bei stark flektierenden Sprachen wie dem Deutschen ist das oft n\u00fctzlich. Diesen Schritt nennt man in der Computerlinguistik Lemmatisierung."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print u' -- '.join(blob.words[0:20].lemmatize())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vor -- 30 -- Jahren -- sein -- Sie -- ein -- der -- Geburtshelfer -- des -- privat -- Rundfunks -- Ihre -- Wort -- von -- damals -- sein -- legend\u00e4r -- Deshalb -- m\u00f6gen -- ich\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Man kann sehen, dass der Schritt f\u00fcr Verben gut funktioniert. So ist aus \u201ewaren\u201c \u201esein\u201c geworden und aus \u201em\u00f6chte\u201c \u201em\u00f6gen\u201c. Bei Substantiven scheint dieser Algorithmus jedoch an seine Grenzen zu kommen: \u201eJahren\u201c ist nicht zu \u201eJahr\u201c geworden und \u201eRundfunks\u201c nicht zu \u201eRundfunk\u201c. In Anwendungsf\u00e4llen, in denen eine hohe Qualit\u00e4t der Verarbeitung wichtig ist, muss man also ggf. nach besseren Werkzeugen suchen oder die vorhandenen verbessern.\n",
      "\n",
      "F\u00fcr speziellere Anwendungen kann man auch die Grammatik eines Texts analysieren. Ein grundlegender Schritt ist dabei die Bestimming der Wortart (Substantiv, Verb, etc.), englisch \u201ePart of Speech\u201c. Hierf\u00fcr werden in der Linguistik meist bestimmte K\u00fcrzel verwendet, die von `TextBlob` sind auf [dieser Seite](http://www.clips.ua.ac.be/pages/mbsp-tags) aufgelistet."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print u' -- '.join([u'{}/{}'.format(word, tag) for word, tag in blob.pos_tags[0:20]])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "vor/IN -- 30/CD -- Jahren/NN -- waren/VB -- Sie/PRP -- einer/DT -- der/DT -- Geburtshelfer/NN -- des/DT -- privaten/JJ -- Rundfunks/NN -- Ihre/PRP$ -- Worte/NNS -- von/IN -- damals/RB -- sind/VB -- legend\u00e4r/JJ -- Deshalb/RB -- m\u00f6chte/VB -- ich/PRP\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dies kann etwa dazu verwendet werden, f\u00fcr bestimmte Anwendungen nur bestimmte Wortarten zu ber\u00fccksichtigen. Falls etwa nur die Substantive interessieren, lassen sie sich aufgrund der PoS-Information herausfiltern.\n",
      "\n",
      "Um Informationen wie Lemma und Wortart nicht einzeln erzeugen zu m\u00fcssen, kann man diese mit der `parse()`-Funktion auch in einem Durchlauf erzeugen. Da der Standard-Parser keine Lemmata erzeugt, m\u00fcssen diese zun\u00e4chst explizit aktiviert werden."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from textblob_de import PatternParser\n",
      "blob = TextBlob(rede, parser=PatternParser(lemmata=True))\n",
      "parse = blob.parse()\n",
      "parse[0:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "u'vor/IN/B-PP/B-PNP/vor 30/CD/B-NP/I-PNP/30 Jahren/NN/I-NP/I-PNP/jahren waren/VB/B-VP/O/sein Sie/PRP/B'"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wie man sieht, werden hier alle Informationen zu einem Wort durch Schr\u00e4gstriche getrennt angegeben. Dies ist eine \u00fcbliche Konvention in der Linguistik. Die Bedeutung der einzelnen Tags kann man sich anzeigen lassen:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "parse.tags"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "[u'word', u'part-of-speech', u'chunk', u'preposition', u'lemma']"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In dieser Form sind die Angaben nicht sehr leicht zu entziffern. Man kann sie aber leicht in eine besser zu verarbeitende Form \u00fcberf\u00fchren. In Python steht daf\u00fcr `namedtuple()` zur Verf\u00fcgung, mit dem man einzelne Felder in einer Liste (nichts anderes ist ein sogenanntes \u201etuple\u201c im Grunde) benennen kann.\n",
      "\n",
      "Da Feldnamen in Python kein '-' enthalten d\u00fcrfen, wird es zun\u00e4chst durch einen Unterstrich ersetzt. Auf der Grundlage k\u00f6nnen wir eine neue Datenstruktur (Klasse) erzeugen, die einen einfachen Zugriff auf die einzelnen Informationen erlaubt."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import namedtuple\n",
      "fieldnames = [tag.replace('-', '_') for tag in parse.tags]\n",
      "Token = namedtuple('Token', fieldnames)\n",
      "Token('vor', 'IN', 'B-PP', 'B-PNP', 'vor')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "Token(word='vor', part_of_speech='IN', chunk='B-PP', preposition='B-PNP', lemma='vor')"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Der Klasse `Token` m\u00fcssen die Informationen als einzelne Argumente \u00fcbergeben werden. Das ist dann ein Problem, wenn die Informationen als Liste in einer einzelnen Variable vorliegen."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fields = ['vor', 'IN', 'B-PP', 'B-PNP', 'vor']\n",
      "Token(fields)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "TypeError",
       "evalue": "__new__() takes exactly 6 arguments (2 given)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-10-454c823e1975>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mfields\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'vor'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'IN'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'B-PP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'B-PNP'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vor'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mToken\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfields\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: __new__() takes exactly 6 arguments (2 given)"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Die Klasse h\u00e4tte f\u00fcnf Argumente erwartet, hat aber nur eines, n\u00e4mlich die Liste mit f\u00fcnf Elementen, erhalten. (Ein Argument wird immer intern verwendet, daher spricht die Fehlermeldung von 6 und 2 statt von 5 und 1.) Mit `*` lassen sich aber Listen statt einzelner Argumente \u00fcbergeben."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Token(*fields)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "Token(word='vor', part_of_speech='IN', chunk='B-PP', preposition='B-PNP', lemma='vor')"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dies kann man sich nun zunutze machen, um aus dem \u201eParse\u201c des Textes eine besser zu verarbeitende Datenstruktur zu gewinnen. Da einzelne Worte durch Leerzeichen und die einzelnen Informationen pro Wort durch Schr\u00e4gstriche getrennt sind, muss der Parse nur zweimal aufgespalten werden. Die dadurch gewonnenen Einzelinformationen pro Wort werden in der Token-Klasse gespeichert."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = [Token(*token.split('/')) for token in parse.split(' ')]\n",
      "tokens[0:5]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[Token(word=u'vor', part_of_speech=u'IN', chunk=u'B-PP', preposition=u'B-PNP', lemma=u'vor'),\n",
        " Token(word=u'30', part_of_speech=u'CD', chunk=u'B-NP', preposition=u'I-PNP', lemma=u'30'),\n",
        " Token(word=u'Jahren', part_of_speech=u'NN', chunk=u'I-NP', preposition=u'I-PNP', lemma=u'jahren'),\n",
        " Token(word=u'waren', part_of_speech=u'VB', chunk=u'B-VP', preposition=u'O', lemma=u'sein'),\n",
        " Token(word=u'Sie', part_of_speech=u'PRP', chunk=u'B-NP', preposition=u'O', lemma=u'sie')]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def lemmatize_and_filter(tokens, tags):\n",
      "    result = []\n",
      "    for token in tokens:\n",
      "        pos = token.part_of_speech[0:2]\n",
      "        if pos in tags:\n",
      "            if pos == 'NN':\n",
      "                # Substantive immer gro\u00df schreiben\n",
      "                result.append(token.lemma.title())\n",
      "            else:\n",
      "                restult.append(token.lemma)\n",
      "    return result\n",
      "\n",
      "print u' -- '.join(lemmatize_and_filter(tokens, ['NN'])[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jahren -- Geburtshelfer -- Rundfunks -- Wort -- Kellerstudio -- Ludwigshafen -- Zuschauer -- Morgen -- Januar -- Worten -- \u201eMeine -- Damen -- Herren -- Moment -- Zeug -- Starts -- Fernsehveranstalters -- Bundesrepublik -- Deutschland -- Zeugen\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Das Verfahren funktioniert in weiten Teilen, aber alle Methoden der Computerlinguistik sind mit einer gewissen Fehlerrate behaftet. Gute Verfahren haben dabei eine Genauigkeit von \u00fcber 90% \u2013 was in der Gesamtschau immer noch eine Menge Fehler sind. In diesem Fall sieht man, dass der Wortfinde-Algorithmus nicht mit den deutschen Anf\u00fchrungszeichen umgehen kann. Hier w\u00fcrde eine entsprechende Vorbereitung des Textes helfen.\n",
      "\n",
      "Ein anderer Ansatz kann sein, nicht \u00fcber die Wortarten, sondern \u00fcber eine vorgegebene Liste an W\u00f6rtern zu filtern. Je nach Anwendungszweck kann eine solche Stoppwortliste l\u00e4nger oder k\u00fcrzer sein. Ihr Ziel ist es, nicht bedeutungstragende W\u00f6rter herauszufiltern. Dies k\u00f6nnen neben Partikeln auch Hilfsverben und unspezifische Adjektive und Adverben (z.B. \u201esehr\u201c) sein."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "with codecs.open('stopwords.txt', 'r', 'utf-8') as stopwordfile:\n",
      "    stopwords_de = [line.strip() for line in stopwordfile.readlines()]\n",
      "\n",
      "from string import punctuation\n",
      "\n",
      "def lemmatize_and_filter2(tokens, stopwords):\n",
      "    result = []\n",
      "    for token in tokens:\n",
      "        pos = token.part_of_speech[0:2]\n",
      "        word = token.word\n",
      "        if not word.lower() in stopwords and not word.isdigit() and not word in punctuation:\n",
      "            if pos == 'NN':\n",
      "                result.append(token.lemma.title())\n",
      "            else:\n",
      "                result.append(token.lemma)\n",
      "    return result\n",
      "\n",
      "print u' -- '.join(lemmatize_and_filter2(tokens, stopwords_de)[0:20])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Jahren -- Geburtshelfer -- privat -- Rundfunks -- Wort -- legend\u00e4r -- zitieren -- Kellerstudio -- Ludwigshafen -- begr\u00fc\u00dfen -- Zuschauer -- Januar -- Worten -- \u201eMeine -- verehrt -- Damen -- Herren -- Moment -- Zeug -- Starts\n"
       ]
      }
     ],
     "prompt_number": 14
    }
   ],
   "metadata": {}
  }
 ]
}