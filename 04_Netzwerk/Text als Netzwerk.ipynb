{
 "metadata": {
  "name": "",
  "signature": "sha256:4716b046131e7d4e171af573c1deb44df54983d7f1a86c87f8804a0c48dddc7d"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Text als Netzwerk"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Die computerlinguistische Aufbereitung von Texten kann als Ausgangspunkt f\u00fcr weitere Analysen dienen, die typische Muster und Inhalte in Texten sichtbar machen. Eine Grundidee ist dabei, dass Elemente sich nicht allein durch ihre individuellen Eigenschaften auszeichnen, sondern vor allem durch ihre Relationen zu anderen Elementen. In der Soziologie hat dies etwa Mustafa Emirbayer (1997) herausgestellt. Dies gilt aber nicht nur f\u00fcr soziale Akteure, sondern auch f\u00fcr Sprache selbst. So hat etwa bereits Ludwig Wittgenstein in seinen *Philosophischen Untersuchungen* (2008 [1953]) die Verwandtschaft von Begriffen als ein \u201ekompliziertes Netz von \u00c4hnlichkeiten\u201c (ebd., 57) beschrieben.\n",
      "\n",
      "Diese abstrakten Ideen lassen sich mit den Methoden der Netzwerkanalyse operationalisieren. Ein Netzwerk ist dabei zun\u00e4chst eine M\u00f6glichkeit, Beziehungen zwischen Elementen abzubilden. Die g\u00e4ngigen Attributdaten der Soziologie werden zumeist als Tabelle dargestellt, bei der jede Zeilen einen Fall und jede Spalte dessen Eigenschaften repr\u00e4sentiert. Im Netzwerk kommt eine zweite Ebene hinzu: Die Beziehungen zwischen diesen F\u00e4llen, wobei die einzelnen Verbindungen wieder Eingenschaften haben k\u00f6nnen (etwa Art, Intensit\u00e4t, Dauer, etc.).\n",
      "\n",
      "In der Sozialforschung werden in der Regel Netzwerke von Akteuren analysiert. Auch solche Netzwerke lassen sich prinzipiell aus Texten gewinnen (etwa Diesner und Carley 2005). F\u00fcr die Analyse von Texten lassen sich aber auch inhaltliche (semantische) Aspekte als Netzwerk darstellen und analysieren."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import networkx as nx\n",
      "import pandas as pd\n",
      "from textblob_de import TextBlobDE as TextBlob\n",
      "from itertools import combinations\n",
      "from pprint import pprint"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Da wir hierf\u00fcr die Verfahren der computerlinguistischen Sprachverarbeitung nutzen k\u00f6nnen, definieren wir zun\u00e4chst wieder eine Funktion zur Lemmatisierung und zur Part-of-Speech-Filterung, die die Funktionen von `TextBlob` nutzt.\n",
      "\n",
      "Die W\u00f6rter werden dabei nach drei Kriterien gefiltert:\n",
      "\n",
      "* POS-Tags (wobei nur die ersten beiden Buchstaben der Tags ber\u00fccksichtigt werden, die den Oberkategorien entsprechen, z.B. 'NN' f\u00fcr Substantive),\n",
      "* eine Stopwortliste,\n",
      "* und ein regul\u00e4rer Ausdruck, der dazu dient, Zahlen enthaltende W\u00f6rter (z.B. '1990er', '18j\u00e4hrige') zu entfernen."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import re\n",
      "import codecs\n",
      "with codecs.open('../Daten/stopwords.txt', 'r', 'utf-8') as stopwordfile:\n",
      "    stopwords = [line.strip() for line in stopwordfile.readlines()]\n",
      "\n",
      "def lemmatize_and_filter(blob, tags):\n",
      "    lemmas = blob.words.lemmatize()\n",
      "    tagged = blob.pos_tags\n",
      "    result = []\n",
      "    for lemma, (word, tag) in zip(lemmas, tagged):\n",
      "        if tag[0:2] in tags and not word in stopwords and re.match(u'[^\\W0-9]+', lemma, re.U):\n",
      "            result.append(lemma)\n",
      "    return result"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ein einfacher Ansatz, Relationen von Texteinheiten (W\u00f6rtern) zu bestimmen, ist die Kookkurrenz: Zwei W\u00f6rter, die in r\u00e4umlicher N\u00e4he zueinander stehen, haben eine erh\u00f6hte Wahrscheinlichkeit, auch inhaltlich aufeinander bezogen zu sein. Als r\u00e4umliche N\u00e4he kann z.B. ein Satz definiert werden. Es sind aber auch andere Ans\u00e4tze m\u00f6glich, etwa ein fester Wortabstand von etwa 5 W\u00f6rtern. Alle (gefilterten) W\u00f6rter im jeweiligen Fenster sollen also als vernetzt gelten. Dazu muss f\u00fcr jede Zweierkombination aller W\u00f6rter im Fenster eine Relation erzeugt werden.\n",
      "\n",
      "In Python steht daf\u00fcr die Funktion `combinations()` zur Verf\u00fcgung:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "[(a, b) for a, b in combinations('abc', 2)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "[('a', 'b'), ('a', 'c'), ('b', 'c')]"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Die Python-Bibliothek [NetworkX](http://networkx.github.io/) stellt einfache Funktionen bereit, ein Netzwerk zu erzeugen. Sie ist dabei allerdings nicht besonders schnell. F\u00fcr gr\u00f6\u00dfere Projekte bieten sich andere Bibliotheken wie [igraph](http://igraph.org/python/) an.\n",
      "\n",
      "Das Kernvorgehen ist dabei sehr einfach:\n",
      "\n",
      "1. Das Corpus ist in einer CSV-Datei gespeichert, wobei die Spalte 'text' den eigentlichen Text enh\u00e4lt.\n",
      "2. F\u00fcr jeden Text wird ein `TextBlob` erzeugt, der linguistische Informationen erg\u00e4nzt, so u.a. auch Satzgrenzen.\n",
      "3. F\u00fcr jeden Satz werden die zu ber\u00fccksichtigenden Lemmata bestimmt. In diesem Fall werden nur Substantive und Adjektive einbezogen.\n",
      "4. Es werden f\u00fcr jeden Satz alle m\u00f6glichen Kombinationen dieser Lemmata ermittelt.\n",
      "5. Falls eine bestimmte Relation schon im Netzwerk enthalten ist, wird ihr \u201eGewicht\u201c um 1 erh\u00f6ht, andernfalls wird eine neue Relation mit dem Gewicht 1 hinzugef\u00fcgt."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data = pd.read_csv('../Daten/reden.csv', parse_dates=['date'], encoding='utf-8')\n",
      "graph = nx.Graph()\n",
      "for text in data['text']:\n",
      "    blob = TextBlob(text)\n",
      "    for sentence in blob.sentences:\n",
      "        lemmas = lemmatize_and_filter(sentence, ('NN', 'JJ'))\n",
      "        for a, b in combinations(lemmas, 2):\n",
      "            if a == b:\n",
      "                continue\n",
      "            try:\n",
      "                graph[a][b]['weight'] += 1\n",
      "            except KeyError:\n",
      "                graph.add_edge(a, b, weight=1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "u'Nodes: {}, Edges: {}'.format(graph.number_of_nodes(), graph.number_of_edges())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 5,
       "text": [
        "u'Nodes: 46085, Edges: 952228'"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Um diese Prozedur nicht f\u00fcr jede Analyse wiederholen zu m\u00fcssen, kann das Ergebnis in einer Datei gespeichert werden."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "nx.write_graphml(graph, '../Daten/network.graphml')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Dieses Netzwerk kann nun mit Mitteln der Netzwerkanalyse analysiert werden. Eine einfache Netzwerkstatistik ist der \u201cdegree\u201d, die Anzahl der Verbindungen jedes Knotens. `graph.degree()` aus NetworkX gibt dabei in einem `dict` f\u00fcr jeden Knoten seinen degree aus. Die Python-Klasse `Counter` stellt f\u00fcr solche dictionaries einige Zusatzfunktionen bereit, etwa die Ausgabe der `n` Knoten mit dem h\u00f6chsten degree."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import Counter\n",
      "nodes_by_degree = Counter(graph.degree())\n",
      "tops = nodes_by_degree.most_common(20)\n",
      "tops"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "[(u'Deutschland', 7821),\n",
        " (u'gross', 6254),\n",
        " (u'Menschen', 5721),\n",
        " (u'deutsch', 5360),\n",
        " (u'Jahren', 5282),\n",
        " (u'Herren', 5118),\n",
        " (u'Damen', 5087),\n",
        " (u'Deutsch', 4655),\n",
        " (u'Jahr', 4579),\n",
        " (u'Beispiel', 4578),\n",
        " (u'Herr', 4256),\n",
        " (u'Europa', 4239),\n",
        " (u'Welt', 3916),\n",
        " (u'lieb', 3705),\n",
        " (u'Zeit', 3594),\n",
        " (u'Jahre', 3507),\n",
        " (u'Frage', 3494),\n",
        " (u'Land', 3445),\n",
        " (u'europ\\xe4isch', 3409),\n",
        " (u'international', 3396)]"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Das Gesamtnetzwerk mit seinen 46.000 Knoten ist dabei nicht leicht zu analysieren und inhaltlich zu interpretieren. Es kann aber ein Ausgangspunkt f\u00fcr Detailanalysen sein. Ausgehend von der Annahme, dass sich die Bedeutung eines Wortes durch seinen Verwendungszusammenhang ergibt, lassen sich etwa Begriffsanalysen durchf\u00fchren. Hierf\u00fcr kann f\u00fcr ein Wort ein Teilnetzwerk erstellt werden, das nur die mit ihm direkt verbundene W\u00f6rter (\u201eNachbarn\u201c) und deren Verbindungen untereinander enth\u00e4lt.\n",
      "\n",
      "Als Beispiel soll das Wort \u201eIntegration\u201c dienen."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subgraph = graph.subgraph(graph.neighbors(u'Integration'))\n",
      "u'Nodes: {}, Edges: {}'.format(subgraph.number_of_nodes(), subgraph.number_of_edges())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 8,
       "text": [
        "u'Nodes: 861, Edges: 48052'"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Da eher zuf\u00e4llige Verbindungen das Ergebnis verzerren und ja systematische Beziehungen analysiert werden sollen, k\u00f6nnen die Kanten im Netzwerk noch gefiltert werden. Hier werden alle Kanten, die ein Gewicht von 1 haben, gel\u00f6scht. Es k\u00f6nnen auch h\u00f6here Schwellenwerte festgelegt oder aber ausgefeiltere statistische Kriterien zugrundegelegt werden, die die Signifikanz von Verbindungen bestimmen."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lowedges = [edge for edge in subgraph.edges(data=True) if edge[2]['weight'] < 2]\n",
      "len(lowedges)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 9,
       "text": [
        "24808"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Durch das Entfernen von Kanten k\u00f6nnen im Netzwerk \u201eInseln\u201c entstehen, die nicht mehr mit dem Gesamtnetzwerk verbunden sind:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subgraph.remove_edges_from(lowedges)\n",
      "nx.number_connected_components(subgraph)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 10,
       "text": [
        "122"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Diese werden f\u00fcr die weitere Analyse ausgeschlossen.\n",
      "\n",
      "*Hinweis:* Hier wird ein Kniff angewandt, der m\u00f6glichst ressourcensparend arbeitet, aber nicht auf den ersten Blick eing\u00e4ngig ist. Daher hier eine kurze Erl\u00e4uterung.\n",
      "\n",
      "Die Funktion `connected_component_subgraphs` gibt f\u00fcr jede verbundene Komponente des Netzwerks ein Teilnetzwerk zur\u00fcck. Da dies aber rechenaufw\u00e4ndig ist und potenziell viel Speicher ben\u00f6tigt, ist das Ergebnis der Funktion keine Liste aller Teilnetzwerke, sondern ein \u201eGenerator\u201c. Ein Generator unterscheidet sich von einer Liste dadurch, dass seine Elemente erst \u201egeneriert\u201c werden, wenn auf sie zugegriffen wird. Dabei erlaubt er nur die Form `for x in y`, nicht aber `y[i]`. Ein Generator kann aber in eine Liste umgewandelt werden: `list(y)`.\n",
      "\n",
      "Die Komponenten in `connected_component_subgraphs` sind nach ihrer Gr\u00f6\u00dfe geordnet, wobei das gr\u00f6\u00dfte Teilnetzwerk zuerst ausgegeben wird. Um also nur das gr\u00f6\u00dfte Teilnetzwerk zu erhalten w\u00e4re grunds\u00e4tzlich dieses Vorgehen m\u00f6glich (und auch nachvollziehbarer):\n",
      "\n",
      "```python\n",
      "subgraphs = list(nx.connected_component_subgraphs(subgraph))\n",
      "subgraph2 = subgraphs[0]\n",
      "```\n",
      "\n",
      "Daf\u00fcr m\u00fcssten aber zun\u00e4chst alle Teilnetzwerke erzeugt und in der Liste `subgraphs` gespeichert werden, obwohl sie nicht ben\u00f6tigt werden. Daher kann hier ein anderer Weg gew\u00e4hlt werden: In der Schleife `for subgraph2 in nx_connected_component_subgraphs(subgraph):` wird im ersten Durchgang das erste Teilnetzwerk erzeugt und der Variable `subgraph2` zugewiesen. Da das alles ist, was wir ben\u00f6tigen, kann die Schleife direkt danach abgebrochen werden, ohne dass weitere Befehle ausgef\u00fchrt werden."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for subgraph2 in nx.connected_component_subgraphs(subgraph):\n",
      "    break\n",
      "u'Nodes: {}, Edges: {}'.format(subgraph2.number_of_nodes(), subgraph2.number_of_edges())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "u'Nodes: 739, Edges: 23243'"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "F\u00fcr die Analyse des Begriffsnetzwerkes von \u201eIntegration\u201c sind diejenigen Bereiche von Interesse, die eine dichtere Vernetzung aufweisen. Diese Teilnetzwerke sind nicht \u201eKomponenten\u201c im oben genannten Sinne, da sie durchaus Teil des Gesamtnetzwerkes sind. Sie zu identifizieren kann also nur n\u00e4herungsweise geschehen, es gibt kein absolutes Ergebnis. Dementsprechend sind in den letzten Jahren zahlreiche Algorithmen zur sogenannten \u201ccommunity detection\u201d entwickelt worden, die sich im Detail und in ihrer Effizienz unterscheiden. Ein etablierter Algorithmus f\u00fcr NetworkX ist in [diesem Modul](http://perso.crans.org/aynaud/communities/) verf\u00fcgbar.\n",
      "\n",
      "*Hinweis:* Die Funktion `best_partition()` gibt f\u00fcr jeden Knoten im Netzwerk die Nummer seiner Community zur\u00fcck:\n",
      "\n",
      "    {'node1': 0,\n",
      "     'node2': 1,\n",
      "     'node3': 0}\n",
      "\n",
      "Diese Form ist aber nur bedingt geeignet, um die Communities weiter zu analysieren. Besser geeignet w\u00e4re eine Liste aller Knoten f\u00fcr jede Community:\n",
      "\n",
      "    [['node1', 'node3'], ['node2']]\n",
      "\n",
      "Daher wird zun\u00e4chst eine Liste erzeugt, die f\u00fcr jeden m\u00f6glichen Community-Wert eine leere Liste enth\u00e4lt. Im n\u00e4chsten Schritt wird jeder Knoten der seiner Community entsprechenden Liste hinzugef\u00fcgt. Dadurch wird die gew\u00fcnschte Struktur hergestellt.\n",
      "\n",
      "Im n\u00e4chsten Schritt kann dann f\u00fcr jede Community ein Teilnetzwerk erzeugt werden. Der Degree der Knoten in den Teilnetzwerken gibt Aufschluss \u00fcber die zentralen Begriffe dieser Teilnetzwerke."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import community\n",
      "partition = community.best_partition(subgraph2)\n",
      "\n",
      "communities = [[] for i in set(partition.values())]\n",
      "for node, comm in partition.items():\n",
      "    communities[comm].append(node)\n",
      "    \n",
      "community_graphs = [subgraph2.subgraph(nodes) for nodes in communities]\n",
      "for i, graph in enumerate(community_graphs):\n",
      "    print u'Community {}:'.format(i)\n",
      "    pprint(Counter(graph.degree()).most_common(10))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Community 0:\n",
        "[(u'Europa', 124),\n",
        " (u'europ\\xe4isch', 116),\n",
        " (u'Europ\\xe4isch', 107),\n",
        " (u'Welt', 106),\n",
        " (u'Beispiel', 99),\n",
        " (u'gemeinsam', 98),\n",
        " (u'Union', 97),\n",
        " (u'L\\xe4nder', 96),\n",
        " (u'L\\xe4ndern', 87),\n",
        " (u'Weg', 81)]\n",
        "Community 1:\n",
        "[(u'Menschen', 146),\n",
        " (u'Frage', 119),\n",
        " (u'Zukunft', 116),\n",
        " (u'Gesellschaft', 113),\n",
        " (u'Politik', 94),\n",
        " (u'Thema', 91),\n",
        " (u'Leben', 89),\n",
        " (u'Aufgabe', 88),\n",
        " (u'politisch', 88),\n",
        " (u'wichtig', 86)]\n",
        "Community 2:\n",
        "[(u'Deutschland', 147),\n",
        " (u'deutsch', 114),\n",
        " (u'gross', 106),\n",
        " (u'Jahren', 96),\n",
        " (u'Land', 90),\n",
        " (u'Jahre', 77),\n",
        " (u'Zeit', 77),\n",
        " (u'international', 77),\n",
        " (u'Geschichte', 76),\n",
        " (u'Seite', 66)]\n",
        "Community 3:\n",
        "[(u'Herr', 91),\n",
        " (u'lieb', 89),\n",
        " (u'Herren', 78),\n",
        " (u'Damen', 78),\n",
        " (u'Deutsch', 75),\n",
        " (u'Frau', 67),\n",
        " (u'herzlich', 57),\n",
        " (u'Professor', 50),\n",
        " (u'freue', 46),\n",
        " (u'Berlin', 43)]\n",
        "Community 4:\n",
        "[(u'Jahr', 24),\n",
        " (u'Bundesregierung', 22),\n",
        " (u'Euro', 20),\n",
        " (u'Rahmen', 17),\n",
        " (u'Bildung', 17),\n",
        " (u'Millionen', 15),\n",
        " (u'Bereich', 15),\n",
        " (u'kulturell', 15),\n",
        " (u'zus\\xe4tzlich', 15),\n",
        " (u'Milliarde', 13)]\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Diese Ausgabe gibt einen ersten Anhaltspunkt f\u00fcr die Interpretation des Begriffs \u201eIntegration\u201c:\n",
      "\n",
      "1. Die erste Community umfasst den Bereich der europ\u00e4ischen Integration.\n",
      "2. Die zweite Community verweist auf Integration als gesellschaftliche Aufgabe und Zukunftsfrage.\n",
      "3. Die dritte Community bezieht sich auf die deutsche Geschichte, ist aber zun\u00e4chst nicht weiter inhaltlich bestimmt. Hier k\u00f6nnte noch eine detailliertere Analyse vorgenommen werden.\n",
      "4. Die vierte Community ist ein Artefakt, das sich aus den in allen Reden enthaltenen Anreden ergibt. Dies k\u00f6nnte ein Hinweis darauf sein, die Anreden vor der Analyse aus den Texten zu entfernen.\n",
      "5. Die f\u00fcnfte Community bestimmt Integration in Hinblick auf politische Investitionen in Bildung und Kultur.\n",
      "\n",
      "Dies gibt einen ersten Eindruck davon, in welchen Kontexten Integration in politischen Reden der aktuellen deutschen Bundesregierung thematisiert wird."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Literatur"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Diesner, Jana und Kathleen M. Carley (2005): \u201eRevealing social structure from texts: meta-matrix text analysis as a novel method for network text analysis\u201c, Causal mapping for information systems and technology research: Approaches, advances, and illustrations , S.\u00a081\u2013108, http://andrew.cmu.edu/user/jdiesner/publications/DiesnerCarley_meta_matrix_text_analysis.pdf (zugegriffen am 8.9.2013).\n",
      "\n",
      "Emirbayer, Mustafa (1997): \u201eManifesto for a relational sociology\u201c, American Journal of Sociology 103/2, S.\u00a0281\u2013317, http://www.jstor.org/stable/10.1086/231209 (zugegriffen am 29.10.2013).\n",
      "\n",
      "Wittgenstein, Ludwig (2008): Philosophische Untersuchungen, hg. von. Joachim Schulte, Bibliothek Suhrkamp\u00a01372, Frankfurt am Main: Suhrkamp."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}